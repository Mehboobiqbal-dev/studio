// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Implements an AI-assisted content moderation flow to flag toxic content for admin review.
 *
 * - moderateContent - A function that moderates content and returns a toxicity score and explanation.
 * - ModerateContentInput - The input type for the moderateContent function.
 * - ModerateContentOutput - The return type for the moderateContent function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const ModerateContentInputSchema = z.object({
  text: z.string().describe('The text content to be moderated.'),
});
export type ModerateContentInput = z.infer<typeof ModerateContentInputSchema>;

const ModerateContentOutputSchema = z.object({
  isToxic: z.boolean().describe('Whether the content is considered toxic.'),
  toxicityScore: z
    .number()
    .describe('A score indicating the level of toxicity (0-1, higher is more toxic).'),
  explanation: z.string().describe('An explanation of why the content is considered toxic.'),
});
export type ModerateContentOutput = z.infer<typeof ModerateContentOutputSchema>;

export async function moderateContent(input: ModerateContentInput): Promise<ModerateContentOutput> {
  return moderateContentFlow(input);
}

const moderateContentPrompt = ai.definePrompt({
  name: 'moderateContentPrompt',
  input: {schema: ModerateContentInputSchema},
  output: {schema: ModerateContentOutputSchema},
  prompt: `You are an AI content moderation assistant. Your task is to analyze the given text content and determine if it is toxic, and to provide an explanation for your assessment.

Text Content: {{{text}}}

Based on the content above, determine:
1. isToxic: Is the content toxic, harmful, or inappropriate? (true/false)
2. toxicityScore: Provide a toxicity score between 0 and 1 (higher means more toxic).
3. explanation: Explain why you consider the content toxic or not, citing specific parts of the text.

Ensure your response adheres to the schema: {isToxic: boolean, toxicityScore: number, explanation: string}.',
  config: {
    safetySettings: [
      {category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_ONLY_HIGH'},
      {category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE'},
      {category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_MEDIUM_AND_ABOVE'},
      {category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_LOW_AND_ABOVE'},
    ],
  },
});

const moderateContentFlow = ai.defineFlow(
  {
    name: 'moderateContentFlow',
    inputSchema: ModerateContentInputSchema,
    outputSchema: ModerateContentOutputSchema,
  },
  async input => {
    const {output} = await moderateContentPrompt(input);
    return output!;
  }
);
